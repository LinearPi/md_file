#kaggle比赛大致流程 

### General Approach

1. ####Data Exploration 数据探索

   用 **pandas** 来载入数据，并做一些简单的可视化来理解数据。 

2. ####Visualization 可视化

    通常来说 **matplotlib** 和 **seaborn** 提供的绘图功能就可以满足需求了。 

    比较常用的图表有：

    - 查看目标变量的分布。当分布[不平衡](http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5128907)时，根据评分标准和具体模型的使用不同，可能会严重影响性能。
    - 对 **Numerical Variable**，可以用 **Box Plot** 来直观地查看它的分布。
    - 对于坐标类数据，可以用 **Scatter Plot** 来查看它们的分布趋势和是否有离群点的存在。
    - 对于分类问题，将数据根据 Label 的不同着不同的颜色绘制出来，这对 Feature 的构造很有帮助。
    - 绘制变量之间两两的分布和相关度图表。

3. ####Statistical Tests 统计学检验 

    ​    我们可以对数据进行一些统计上的测试来验证一些假设的显著性。虽然大部分情况下靠可视化就能得到比较明确的结论，但有一些定量结果总是更理想的。不过，在实际数据中经常会遇到非 i.i.d. 的分布。所以要注意测试类型的的选择和对显著性的解释。

    在某些比赛中，由于数据分布比较奇葩或是噪声过强，Public LB 的分数可能会跟 Local CV 的结果相去甚远。可以根据一些统计测试的结果来粗略地建立一个阈值，用来衡量一次分数的提高究竟是实质的提高还是由于数据的随机性导致的。

4. ####Data Preprocessing

    大部分情况下，在构造 Feature 之前，我们需要对比赛提供的数据集进行一些处理。通常的步骤有：

    - 有时数据会分散在几个不同的文件中，需要 Join 起来。
    - 处理 **Missing Data**。
    - 处理 **Outlier**。
    - 必要时转换某些 **Categorical Variable** 的表示方式。
    - 有些 Float 变量可能是从未知的 Int 变量转换得到的，这个过程中发生精度损失会在数据中产生不必要的 **Noise**，即两个数值原本是相同的却在小数点后某一位开始有不同。这对 Model 可能会产生很负面的影响，需要设法去除或者减弱 Noise。

    这一部分的处理策略多半依赖于在前一步中探索数据集所得到的结论以及创建的可视化图表。在实践中，我建议使用 **iPython Notebook** 进行对数据的操作，并熟练掌握常用的 pandas 函数。这样做的好处是可以随时得到结果的反馈和进行修改，也方便跟其他人进行交流。





有人总结 Kaggle 比赛是 **“Feature 为主，调参和 Ensemble 为辅”**，我觉得很有道理。Feature Engineering 能做到什么程度，取决于对数据领域的了解程度。比如在数据包含大量文本的比赛中，常用的 NLP 特征就是必须的。怎么构造有用的 Feature，是一个不断学习和提高的过程。 

